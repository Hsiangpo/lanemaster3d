# 2026-02-19 18:08 GPU利用率冲高阶段计划（暂停快照）

## 1. 当前状态
- 已按你的要求暂停当前动作，训练会话 `lm3d_train` 已停止。
- 服务器当前无 `torchrun/tools/train.py` 训练进程在跑。
- 最近一次目标仍是把双卡训练平均利用率推进到 80%+，并继续冲 90%。

## 2. 我现在在做的事情
- 目标：定位并消除训练中的“长等待数据批次”问题，让 GPU 不再长时间空转。
- 方法：本地改配置/代码 -> `sftp` 推送远端 -> `tmux` 启动新实验 -> 60 秒窗口统计 GPU 平均利用率 -> 不达标立即切下一组。
- 评估标准：
  - 训练必须正常落盘 `experiments/<exp>/logs/metrics.jsonl`。
  - `both_avg`（双卡平均）作为主指标。
  - 同时看 `metrics.jsonl` 中的 `data_time/iter_time/time_forward/time_backward` 来判断瓶颈。

## 3. 已完成动作与结论
- 远端固定入口已接管并持续可用：`ssh -p 2222 sust@38.127.121.195`。
- 新增本地临时执行/同步工具：
  - `.tmp_remote_exec.py`（远端执行）
  - `.tmp_remote_put.py` / `.tmp_remote_get.py`（远端文件同步）
  - `.tmp_gpu_stats.py`（60 秒窗口利用率统计）
- 已做实验：
  - `exp074`（compile关闭、timing拆分开启、gpc/uncertainty每步计算）
  - 采样结果：`gpu0_avg=45.68`，`gpu1_avg=53.00`，`both_avg=49.34`
  - 结论：仍未达标，主要是数据等待阶段导致平均值被拉低。
- 为继续定位，已新增训练开关（尚未继续跑完验证）：
  - 在 `trainer.py` 增加 `runtime.train_shuffle`（默认 True）
  - 目的：做“顺序读取 vs 随机读取”对照，验证是否存在随机 I/O 导致的数据吞吐瓶颈。

## 4. 接下来的执行计划（恢复后按此顺序）
1. 先跑 `exp076`（`train_shuffle=False`）短窗口验证，确认 `both_avg` 是否明显高于 `exp074`。
2. 若提升明显：
   - 保留顺序读取思路，做受控随机（按块打散）以兼顾吞吐与训练随机性。
   - 再叠加 `num_workers/prefetch_factor` 小步调参。
3. 若提升不明显：
   - 回到 `train_shuffle=True`，集中优化数据读取链路（图像解码与批次准备）。
   - 用同样 60 秒窗口和 `data_time` 证据驱动下一轮。
4. 每轮只改 1~2 个变量，不做大杂烩改动，避免结论失真。

## 5. 风险与控制
- 风险1：并发启动/停止命令可能误杀新进程。
  - 控制：停止与启动改为串行执行，不并发。
- 风险2：预热阶段会造成短时低利用率假象。
  - 控制：固定在“进入稳定迭代后”再采 60 秒窗口。
- 风险3：只看瞬时峰值会误判。
  - 控制：统一以 `both_avg` + `p50` 判定，不用单点峰值。

## 6. 关键文件
- 计划文档：`docs/plans/20260219_1808_gpu-util-plan.md`
- 远端训练器（已加开关）：`/home/sust/zhou/lanemaster3d/lanemaster3d/engine/trainer.py`
- 当前实验配置：`/home/sust/zhou/lanemaster3d/configs/openlane/r50_960x720_exp076.py`
